{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "electricity_theft_detection"
   },
   "source": [
    "# Electricity Theft Detection - Training with Real SGCC Data\n",
    "\n",
    "This notebook trains the electricity theft detection model using real SGCC dataset.\n",
    "**IMPORTANT**: Upload your 'datasetsmall.csv' file to Colab before running this notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages\n!pip install --upgrade pip\n!pip install scikit-learn==1.3.2 xgboost==2.0.2 pandas==2.1.4 numpy==1.24.4\n!pip install tsfresh==0.20.2 imbalanced-learn==0.11.0 shap==0.43.0\n!pip install scipy==1.11.4 statsmodels==0.14.1 loguru==0.7.2\n!pip install joblib"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Dict, Optional, List\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from scipy import stats\n",
    "import xgboost as xgb\n",
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Upload Dataset\n",
    "**Please upload your 'datasetsmall.csv' file using the file upload button below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload dataset file\n",
    "print(\"Please upload your SGCC dataset file (datasetsmall.csv)\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Check if file was uploaded\n",
    "if uploaded:\n",
    "    print(f\"Successfully uploaded: {list(uploaded.keys())}\")\n",
    "else:\n",
    "    print(\"No file uploaded. Please upload datasetsmall.csv and re-run this cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Preprocessing Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGCCDataLoader:\n",
    "    \"\"\"\n",
    "    SGCC Dataset Loader for Electricity Theft Detection\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_path: Optional[str] = None):\n",
    "        self.data_path = Path(data_path) if data_path else Path(\"data\")\n",
    "        self.data_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "    def load_real_sgcc_data(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load real SGCC dataset from uploaded file\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"Loading real SGCC dataset...\")\n",
    "            \n",
    "            # Try to find the uploaded dataset file\n",
    "            dataset_files = ['datasetsmall.csv', 'dataset.csv', 'sgcc_data.csv']\n",
    "            df_raw = None\n",
    "            \n",
    "            for filename in dataset_files:\n",
    "                try:\n",
    "                    df_raw = pd.read_csv(filename, low_memory=False)\n",
    "                    print(f\"Successfully loaded dataset from {filename}\")\n",
    "                    break\n",
    "                except FileNotFoundError:\n",
    "                    continue\n",
    "            \n",
    "            if df_raw is None:\n",
    "                print(\"Dataset file not found. Please upload your SGCC dataset file.\")\n",
    "                print(\"Expected format: Wide format with date columns and CONS_NO, FLAG columns\")\n",
    "                return None\n",
    "            \n",
    "            # Validate dataset format\n",
    "            if 'CONS_NO' not in df_raw.columns or 'FLAG' not in df_raw.columns:\n",
    "                raise ValueError(\"Dataset must contain 'CONS_NO' and 'FLAG' columns\")\n",
    "            \n",
    "            # Get dataset statistics\n",
    "            total_meters = len(df_raw)\n",
    "            theft_meters = df_raw['FLAG'].sum()\n",
    "            normal_meters = total_meters - theft_meters\n",
    "            theft_rate = (theft_meters / total_meters) * 100\n",
    "            \n",
    "            print(f\"Dataset loaded successfully:\")\n",
    "            print(f\"  - Total meters: {total_meters:,}\")\n",
    "            print(f\"  - Normal meters: {normal_meters:,} ({100-theft_rate:.1f}%)\")\n",
    "            print(f\"  - Theft meters: {theft_meters:,} ({theft_rate:.1f}%)\")\n",
    "            print(f\"  - Date columns: {len(df_raw.columns) - 2}\")\n",
    "            \n",
    "            return df_raw\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load real SGCC dataset: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def convert_wide_to_long(self, df_raw: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Convert SGCC dataset from wide format to long format\n",
    "        \"\"\"\n",
    "        print(\"Converting wide format to long format...\")\n",
    "        \n",
    "        # Get date columns (all except last 2 which are CONS_NO and FLAG)\n",
    "        date_columns = df_raw.columns[:-2].tolist()\n",
    "        print(f\"Found {len(date_columns)} date columns\")\n",
    "        \n",
    "        # Create a copy and rename identifier columns\n",
    "        df_work = df_raw.copy()\n",
    "        df_work = df_work.rename(columns={'CONS_NO': 'meter_id', 'FLAG': 'label'})\n",
    "        \n",
    "        # Melt the dataframe to convert from wide to long format\n",
    "        df_long = pd.melt(\n",
    "            df_work,\n",
    "            id_vars=['meter_id', 'label'],\n",
    "            value_vars=date_columns,\n",
    "            var_name='date',\n",
    "            value_name='consumption'\n",
    "        )\n",
    "        \n",
    "        print(f\"Melted to {len(df_long):,} records\")\n",
    "        \n",
    "        # Convert date strings to datetime\n",
    "        print(\"Parsing date columns...\")\n",
    "        \n",
    "        def parse_date_column(date_str):\n",
    "            try:\n",
    "                if '/' in str(date_str):\n",
    "                    return pd.to_datetime(date_str, format='%m/%d/%Y', errors='coerce')\n",
    "                else:\n",
    "                    return pd.to_datetime(date_str, errors='coerce')\n",
    "            except:\n",
    "                return pd.NaT\n",
    "        \n",
    "        df_long['date'] = df_long['date'].apply(parse_date_column)\n",
    "        \n",
    "        # Handle any dates that couldn't be parsed\n",
    "        failed_dates_mask = df_long['date'].isna()\n",
    "        if failed_dates_mask.sum() > 0:\n",
    "            print(f\"Warning: Could not parse {failed_dates_mask.sum()} date entries, creating sequential dates\")\n",
    "            \n",
    "            # For unparseable dates, create sequential dates starting from 2014-01-01\n",
    "            start_date = pd.to_datetime('2014-01-01')\n",
    "            \n",
    "            # Create mapping from date column names to actual dates\n",
    "            date_mapping = {}\n",
    "            for i, col in enumerate(date_columns):\n",
    "                date_mapping[col] = start_date + pd.Timedelta(days=i)\n",
    "            \n",
    "            # Apply mapping to failed dates\n",
    "            for idx in df_long[failed_dates_mask].index:\n",
    "                original_date_col = df_long.loc[idx, 'date'] if not pd.isna(df_long.loc[idx, 'date']) else None\n",
    "                # Map back to original column\n",
    "                meter_id = df_long.loc[idx, 'meter_id']\n",
    "                meter_rows = df_long[df_long['meter_id'] == meter_id]\n",
    "                position = list(meter_rows.index).index(idx)\n",
    "                \n",
    "                if position < len(date_columns):\n",
    "                    original_col = date_columns[position]\n",
    "                    df_long.loc[idx, 'date'] = date_mapping[original_col]\n",
    "        \n",
    "        # Convert consumption to numeric\n",
    "        print(\"Converting consumption values to numeric...\")\n",
    "        df_long['consumption'] = pd.to_numeric(df_long['consumption'], errors='coerce')\n",
    "        \n",
    "        # Count zero vs missing consumption\n",
    "        zero_consumption = (df_long['consumption'] == 0).sum()\n",
    "        missing_consumption = df_long['consumption'].isna().sum()\n",
    "        print(f\"Zero consumption readings: {zero_consumption:,}\")\n",
    "        print(f\"Missing consumption readings: {missing_consumption:,}\")\n",
    "        \n",
    "        # Remove rows with missing (NaN) consumption values, but keep zeros\n",
    "        initial_len = len(df_long)\n",
    "        df_long = df_long.dropna(subset=['consumption'])\n",
    "        removed = initial_len - len(df_long)\n",
    "        if removed > 0:\n",
    "            print(f\"Removed {removed:,} rows with missing consumption values\")\n",
    "        \n",
    "        # Sort by meter_id and date\n",
    "        df_long = df_long.sort_values(['meter_id', 'date']).reset_index(drop=True)\n",
    "        \n",
    "        print(f\"Final dataset: {len(df_long):,} records\")\n",
    "        print(f\"Date range: {df_long['date'].min()} to {df_long['date'].max()}\")\n",
    "        print(f\"Unique meters: {df_long['meter_id'].nunique():,}\")\n",
    "        print(f\"Consumption range: {df_long['consumption'].min():.2f} to {df_long['consumption'].max():.2f}\")\n",
    "        \n",
    "        return df_long\n",
    "    \n",
    "    def load_dataset(self) -> pd.DataFrame:\n",
    "        \"\"\"Load dataset - uses real SGCC data\"\"\"\n",
    "        # Load real SGCC data\n",
    "        df_raw = self.load_real_sgcc_data()\n",
    "        if df_raw is None:\n",
    "            raise ValueError(\"Could not load SGCC dataset. Please upload datasetsmall.csv to Colab.\")\n",
    "        \n",
    "        # Convert to long format\n",
    "        df_long = self.convert_wide_to_long(df_raw)\n",
    "        \n",
    "        print(f\"Dataset ready: {len(df_long)} records, {df_long['meter_id'].nunique()} unique meters\")\n",
    "        return df_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElectricityDataPreprocessor:\n",
    "    \"\"\"\n",
    "    Data preprocessing pipeline for electricity consumption data\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.scaler = None\n",
    "        self.preprocessing_stats = {}\n",
    "        \n",
    "    def handle_missing_values(self, df: pd.DataFrame, method: str = 'linear') -> pd.DataFrame:\n",
    "        \"\"\"Handle missing values in consumption data\"\"\"\n",
    "        print(f\"Handling missing values using {method} method...\")\n",
    "        \n",
    "        df_processed = df.copy()\n",
    "        initial_missing = df_processed['consumption'].isnull().sum()\n",
    "        \n",
    "        if initial_missing == 0:\n",
    "            print(\"No missing values found\")\n",
    "            return df_processed\n",
    "        \n",
    "        print(f\"Found {initial_missing} missing values ({initial_missing/len(df)*100:.2f}%)\")\n",
    "        \n",
    "        if method == 'linear':\n",
    "            df_processed = df_processed.sort_values(['meter_id', 'date'])\n",
    "            df_processed['consumption'] = df_processed.groupby('meter_id')['consumption'].transform(\n",
    "                lambda x: x.interpolate(method='linear', limit_direction='both')\n",
    "            )\n",
    "            \n",
    "        final_missing = df_processed['consumption'].isnull().sum()\n",
    "        print(f\"Missing values reduced from {initial_missing} to {final_missing}\")\n",
    "        \n",
    "        return df_processed\n",
    "    \n",
    "    def detect_and_remove_outliers(self, df: pd.DataFrame, method: str = 'zscore') -> pd.DataFrame:\n",
    "        \"\"\"Detect and handle outliers\"\"\"\n",
    "        print(f\"Detecting outliers using {method} method...\")\n",
    "        \n",
    "        df_processed = df.copy()\n",
    "        outliers_processed = 0\n",
    "        \n",
    "        if method == 'zscore':\n",
    "            threshold = 3.0\n",
    "            for meter_id in df_processed['meter_id'].unique():\n",
    "                meter_data = df_processed[df_processed['meter_id'] == meter_id]['consumption']\n",
    "                \n",
    "                if len(meter_data) > 10:  # Need sufficient data points\n",
    "                    z_scores = np.abs(stats.zscore(meter_data.dropna()))\n",
    "                    outlier_mask = z_scores > threshold\n",
    "                    \n",
    "                    if outlier_mask.any():\n",
    "                        mean_val = meter_data.mean()\n",
    "                        std_val = meter_data.std()\n",
    "                        lower_bound = mean_val - threshold * std_val\n",
    "                        upper_bound = mean_val + threshold * std_val\n",
    "                        \n",
    "                        # Cap outliers instead of removing them\n",
    "                        meter_mask = df_processed['meter_id'] == meter_id\n",
    "                        df_processed.loc[meter_mask, 'consumption'] = df_processed.loc[meter_mask, 'consumption'].clip(\n",
    "                            lower=max(0, lower_bound), upper=upper_bound\n",
    "                        )\n",
    "                        \n",
    "                        outliers_processed += outlier_mask.sum()\n",
    "        \n",
    "        print(f\"Processed {outliers_processed} outliers\")\n",
    "        return df_processed\n",
    "    \n",
    "    def preprocess_pipeline(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Complete preprocessing pipeline\"\"\"\n",
    "        print(\"Starting preprocessing pipeline...\")\n",
    "        \n",
    "        # Handle missing values\n",
    "        df_processed = self.handle_missing_values(df)\n",
    "        \n",
    "        # Handle outliers\n",
    "        df_processed = self.detect_and_remove_outliers(df_processed)\n",
    "        \n",
    "        print(\"Preprocessing pipeline completed\")\n",
    "        return df_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEngineer:\n",
    "    \"\"\"\n",
    "    Feature engineering for electricity theft detection\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def create_time_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Create time-based features\"\"\"\n",
    "        print(\"Creating time-based features...\")\n",
    "        \n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        df['year'] = df['date'].dt.year\n",
    "        df['month'] = df['date'].dt.month\n",
    "        df['day_of_week'] = df['date'].dt.dayofweek\n",
    "        df['day_of_year'] = df['date'].dt.dayofyear\n",
    "        df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "        \n",
    "        # Season mapping\n",
    "        df['season'] = df['month'].map({\n",
    "            12: 'winter', 1: 'winter', 2: 'winter',\n",
    "            3: 'spring', 4: 'spring', 5: 'spring',\n",
    "            6: 'summer', 7: 'summer', 8: 'summer',\n",
    "            9: 'autumn', 10: 'autumn', 11: 'autumn'\n",
    "        })\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_statistical_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Create statistical features per meter\"\"\"\n",
    "        print(\"Creating statistical features...\")\n",
    "        \n",
    "        # Calculate rolling statistics\n",
    "        df = df.sort_values(['meter_id', 'date'])\n",
    "        \n",
    "        # Rolling statistics (7-day window)\n",
    "        print(\"  - 7-day rolling statistics...\")\n",
    "        df['consumption_7d_mean'] = df.groupby('meter_id')['consumption'].rolling(window=7, min_periods=1).mean().values\n",
    "        df['consumption_7d_std'] = df.groupby('meter_id')['consumption'].rolling(window=7, min_periods=1).std().values\n",
    "        df['consumption_7d_max'] = df.groupby('meter_id')['consumption'].rolling(window=7, min_periods=1).max().values\n",
    "        df['consumption_7d_min'] = df.groupby('meter_id')['consumption'].rolling(window=7, min_periods=1).min().values\n",
    "        \n",
    "        # Rolling statistics (30-day window)\n",
    "        print(\"  - 30-day rolling statistics...\")\n",
    "        df['consumption_30d_mean'] = df.groupby('meter_id')['consumption'].rolling(window=30, min_periods=1).mean().values\n",
    "        df['consumption_30d_std'] = df.groupby('meter_id')['consumption'].rolling(window=30, min_periods=1).std().values\n",
    "        \n",
    "        # Lag features\n",
    "        print(\"  - Lag features...\")\n",
    "        df['consumption_lag1'] = df.groupby('meter_id')['consumption'].shift(1)\n",
    "        df['consumption_lag7'] = df.groupby('meter_id')['consumption'].shift(7)\n",
    "        \n",
    "        # Fill NaN values with forward fill then backward fill\n",
    "        print(\"  - Filling missing values in features...\")\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        df[numeric_cols] = df[numeric_cols].fillna(method='ffill').fillna(method='bfill')\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_aggregate_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Create aggregate features per meter\"\"\"\n",
    "        print(\"Creating aggregate features...\")\n",
    "        \n",
    "        # Per meter aggregates\n",
    "        meter_aggs = df.groupby('meter_id')['consumption'].agg([\n",
    "            'mean', 'std', 'min', 'max', 'median',\n",
    "            lambda x: x.quantile(0.25),  # Q1\n",
    "            lambda x: x.quantile(0.75),  # Q3\n",
    "        ]).round(4)\n",
    "        \n",
    "        # Add skewness and kurtosis safely\n",
    "        try:\n",
    "            meter_aggs['skew'] = df.groupby('meter_id')['consumption'].skew().round(4)\n",
    "            meter_aggs['kurt'] = df.groupby('meter_id')['consumption'].apply(lambda x: x.kurtosis()).round(4)\n",
    "        except:\n",
    "            meter_aggs['skew'] = 0\n",
    "            meter_aggs['kurt'] = 0\n",
    "        \n",
    "        meter_aggs.columns = ['meter_mean', 'meter_std', 'meter_min', 'meter_max', \n",
    "                              'meter_median', 'meter_q1', 'meter_q3', 'meter_skew', 'meter_kurt']\n",
    "        \n",
    "        # Calculate additional features\n",
    "        meter_aggs['meter_range'] = meter_aggs['meter_max'] - meter_aggs['meter_min']\n",
    "        meter_aggs['meter_iqr'] = meter_aggs['meter_q3'] - meter_aggs['meter_q1']\n",
    "        meter_aggs['meter_cv'] = meter_aggs['meter_std'] / (meter_aggs['meter_mean'] + 1e-8)  # Coefficient of variation with small epsilon\n",
    "        \n",
    "        # Handle any infinite values\n",
    "        meter_aggs = meter_aggs.replace([np.inf, -np.inf], 0)\n",
    "        meter_aggs = meter_aggs.fillna(0)\n",
    "        \n",
    "        # Merge back to main dataframe\n",
    "        df = df.merge(meter_aggs, left_on='meter_id', right_index=True, how='left')\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def engineer_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Complete feature engineering pipeline\"\"\"\n",
    "        print(\"Starting feature engineering...\")\n",
    "        \n",
    "        # Create time features\n",
    "        df = self.create_time_features(df)\n",
    "        \n",
    "        # Create statistical features\n",
    "        df = self.create_statistical_features(df)\n",
    "        \n",
    "        # Create aggregate features\n",
    "        df = self.create_aggregate_features(df)\n",
    "        \n",
    "        print(f\"Feature engineering completed. Total features: {df.shape[1]}\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load and Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "loader = SGCCDataLoader()\n",
    "df = loader.load_dataset()\n",
    "\n",
    "preprocessor = ElectricityDataPreprocessor()\n",
    "df = preprocessor.preprocess_pipeline(df)\n",
    "\n",
    "# Feature engineering\n",
    "engineer = FeatureEngineer()\n",
    "df = engineer.engineer_features(df)\n",
    "\n",
    "print(f\"Dataset shape after preprocessing and feature engineering: {df.shape}\")\n",
    "print(f\"Class distribution:\\n{df['label'].value_counts()}\")\n",
    "print(f\"Theft rate: {(df['label'].sum() / len(df) * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prepare Features for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features for training\n",
    "feature_columns = [\n",
    "    'consumption', 'year', 'month', 'day_of_week', 'day_of_year', 'is_weekend',\n",
    "    'consumption_7d_mean', 'consumption_7d_std', 'consumption_7d_max', 'consumption_7d_min',\n",
    "    'consumption_30d_mean', 'consumption_30d_std', 'consumption_lag1', 'consumption_lag7',\n",
    "    'meter_mean', 'meter_std', 'meter_min', 'meter_max', 'meter_median',\n",
    "    'meter_q1', 'meter_q3', 'meter_skew', 'meter_kurt', 'meter_range', 'meter_iqr', 'meter_cv'\n",
    "]\n",
    "\n",
    "# Encode categorical features\n",
    "df_encoded = pd.get_dummies(df, columns=['season'], prefix='season')\n",
    "\n",
    "# Update feature columns\n",
    "season_cols = [col for col in df_encoded.columns if col.startswith('season_')]\n",
    "feature_columns.extend(season_cols)\n",
    "\n",
    "# Remove any features that don't exist\n",
    "feature_columns = [col for col in feature_columns if col in df_encoded.columns]\n",
    "\n",
    "print(f\"Selected {len(feature_columns)} features for training\")\n",
    "print(f\"Features: {feature_columns}\")\n",
    "\n",
    "X = df_encoded[feature_columns]\n",
    "y = df_encoded['label']\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Missing values in features: {X.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle any remaining missing values\n",
    "X = X.fillna(X.median())\n",
    "print(f\"After filling missing values: {X.isnull().sum().sum()} missing values remain\")\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"Training class distribution:\\n{y_train.value_counts()}\")\n",
    "print(f\"Test class distribution:\\n{y_test.value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle class imbalance with SMOTE\n",
    "print(\"Applying SMOTE for class balancing...\")\n",
    "smote = SMOTE(random_state=42, sampling_strategy=0.5)  # Oversample to 50% ratio\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"After SMOTE - Training set: {X_train_balanced.shape[0]} samples\")\n",
    "print(f\"After SMOTE - Class distribution:\\n{pd.Series(y_train_balanced).value_counts()}\")\n",
    "print(f\"New theft rate in training: {(pd.Series(y_train_balanced).sum() / len(y_train_balanced) * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_balanced)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Feature scaling completed\")\n",
    "print(f\"Training features mean: {np.mean(X_train_scaled):.4f}\")\n",
    "print(f\"Training features std: {np.std(X_train_scaled):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost model\n",
    "print(\"Training XGBoost model...\")\n",
    "\n",
    "# XGBoost parameters optimized for theft detection\n",
    "xgb_params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.1,\n",
    "    'n_estimators': 200,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 1.0,\n",
    "    'random_state': 42,\n",
    "    'eval_metric': 'auc'\n",
    "}\n",
    "\n",
    "model = xgb.XGBClassifier(**xgb_params)\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    X_train_scaled, y_train_balanced,\n",
    "    eval_set=[(X_test_scaled, y_test)],\n",
    "    verbose=50  # Show progress every 50 rounds\n",
    ")\n",
    "\n",
    "print(\"Model training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Model Performance:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"AUC Score: {roc_auc_score(y_test, y_pred_proba):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "print(f\"\\nTrue Negatives: {cm[0,0]}\")\n",
    "print(f\"False Positives: {cm[0,1]}\")\n",
    "print(f\"False Negatives: {cm[1,0]}\")\n",
    "print(f\"True Positives: {cm[1,1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_columns,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 15 Most Important Features:\")\n",
    "print(feature_importance.head(15))\n",
    "\n",
    "# Show feature importance distribution\n",
    "print(f\"\\nFeature importance statistics:\")\n",
    "print(f\"Mean importance: {feature_importance['importance'].mean():.4f}\")\n",
    "print(f\"Max importance: {feature_importance['importance'].max():.4f}\")\n",
    "print(f\"Top 5 features account for {feature_importance.head(5)['importance'].sum():.1%} of total importance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Model and Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model and preprocessing components\n",
    "print(\"Saving model and components...\")\n",
    "\n",
    "# Save the main model\n",
    "joblib.dump(model, 'models/xgb_theft_detection_model.pkl')\n",
    "\n",
    "# Save the scaler\n",
    "joblib.dump(scaler, 'models/feature_scaler.pkl')\n",
    "\n",
    "# Save feature columns\n",
    "joblib.dump(feature_columns, 'models/feature_columns.pkl')\n",
    "\n",
    "# Save model metadata\n",
    "model_metadata = {\n",
    "    'model_type': 'XGBoost',\n",
    "    'features': feature_columns,\n",
    "    'n_features': len(feature_columns),\n",
    "    'training_samples': X_train_balanced.shape[0],\n",
    "    'test_samples': X_test.shape[0],\n",
    "    'test_auc': float(roc_auc_score(y_test, y_pred_proba)),\n",
    "    'model_params': xgb_params,\n",
    "    'feature_importance': feature_importance.head(20).to_dict('records'),  # Top 20 features\n",
    "    'training_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'dataset_info': {\n",
    "        'total_meters': df['meter_id'].nunique(),\n",
    "        'total_records': len(df),\n",
    "        'theft_rate': float((df['label'].sum() / len(df)) * 100),\n",
    "        'date_range': f\"{df['date'].min()} to {df['date'].max()}\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save metadata as JSON\n",
    "import json\n",
    "with open('models/model_metadata.json', 'w') as f:\n",
    "    json.dump([model_metadata], f, indent=2)\n",
    "\n",
    "print(\"Model and components saved successfully!\")\n",
    "print(\"\\nSaved files:\")\n",
    "print(\"- models/xgb_theft_detection_model.pkl\")\n",
    "print(\"- models/feature_scaler.pkl\")\n",
    "print(\"- models/feature_columns.pkl\")\n",
    "print(\"- models/model_metadata.json\")\n",
    "\n",
    "print(f\"\\nModel Performance Summary:\")\n",
    "print(f\"AUC Score: {model_metadata['test_auc']:.4f}\")\n",
    "print(f\"Training samples: {model_metadata['training_samples']:,}\")\n",
    "print(f\"Test samples: {model_metadata['test_samples']:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Download Models for Local Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a zip file with all model components\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "zip_filename = 'electricity_theft_detection_model.zip'\n",
    "\n",
    "with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
    "    # Add all model files\n",
    "    for file_path in ['models/xgb_theft_detection_model.pkl', \n",
    "                      'models/feature_scaler.pkl',\n",
    "                      'models/feature_columns.pkl',\n",
    "                      'models/model_metadata.json']:\n",
    "        if os.path.exists(file_path):\n",
    "            zipf.write(file_path, os.path.basename(file_path))\n",
    "            print(f\"Added {file_path} to zip\")\n",
    "\n",
    "print(f\"\\nModel package created: {zip_filename}\")\n",
    "print(\"This file contains everything needed for local deployment.\")\n",
    "\n",
    "# Show zip file contents\n",
    "with zipfile.ZipFile(zip_filename, 'r') as zipf:\n",
    "    print(\"\\nZip file contents:\")\n",
    "    for info in zipf.infolist():\n",
    "        print(f\"  - {info.filename} ({info.file_size:,} bytes)\")\n",
    "\n",
    "# Download the zip file\n",
    "print(\"\\nDownloading model package...\")\n",
    "files.download(zip_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Test Model Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model with sample data\n",
    "def predict_sample(model, scaler, feature_columns, sample_data):\n",
    "    \"\"\"Test function to predict on sample data\"\"\"\n",
    "    \n",
    "    # Ensure all required features are present\n",
    "    for col in feature_columns:\n",
    "        if col not in sample_data.columns:\n",
    "            sample_data[col] = 0  # Fill missing features with default values\n",
    "    \n",
    "    # Select and reorder features\n",
    "    X_sample = sample_data[feature_columns]\n",
    "    \n",
    "    # Scale features\n",
    "    X_sample_scaled = scaler.transform(X_sample)\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict(X_sample_scaled)\n",
    "    probability = model.predict_proba(X_sample_scaled)[:, 1]\n",
    "    \n",
    "    return prediction, probability\n",
    "\n",
    "# Test with a few samples from test set\n",
    "test_samples = X_test.head(10)\n",
    "test_labels = y_test.head(10)\n",
    "\n",
    "predictions, probabilities = predict_sample(model, scaler, feature_columns, test_samples)\n",
    "\n",
    "print(\"Sample Predictions:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Sample':<8} {'Prediction':<12} {'Probability':<12} {'Actual':<8} {'Result':<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for i, (pred, prob, actual) in enumerate(zip(predictions, probabilities, test_labels)):\n",
    "    result = \"âœ“ Correct\" if pred == actual else \"âœ— Wrong\"\n",
    "    print(f\"{i+1:<8} {pred:<12} {prob:<12.3f} {actual:<8} {result:<10}\")\n",
    "\n",
    "accuracy = sum(predictions == test_labels) / len(test_labels)\n",
    "print(\"-\" * 70)\n",
    "print(f\"Sample accuracy: {accuracy:.1%}\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ Model is ready for deployment!\")\n",
    "print(\"Next steps:\")\n",
    "print(\"1. Download the model package zip file\")\n",
    "print(\"2. Extract it in your local project\")\n",
    "print(\"3. Use the deployment scripts to make predictions\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}